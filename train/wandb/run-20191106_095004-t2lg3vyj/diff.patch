diff --git a/tracking/SiamRPNEval.py b/tracking/SiamRPNEval.py
new file mode 100644
index 0000000..fd12502
--- /dev/null
+++ b/tracking/SiamRPNEval.py
@@ -0,0 +1,160 @@
+import cv2
+import torch
+import numpy as np
+from tracking.util import util
+import torch.nn.functional as F
+import torchvision.transforms as transforms
+from tracking.custom_transforms import ToTensor
+from tracking.config import config
+from got10k.trackers import Tracker
+from tracking.network import SiameseAlexNet, SiameseAlexNetMultimodal
+from tracking.data_loader import TrackerDataLoader
+
+
+class TrackerSiamRPNEval(Tracker):
+
+    def __init__(self, params, modality=1,model_path = None, **kargs):
+        super(TrackerSiamRPNEval, self).__init__(name='SiamRPN', is_deterministic=True)
+
+        self.modality = modality
+
+        if modality == 1:
+            self.model = SiameseAlexNet()
+        else:
+            self.model = SiameseAlexNetMultimodal()
+
+
+        self.cuda = torch.cuda.is_available()
+        self.device = torch.device('cuda:0' if self.cuda else 'cpu')
+
+        checkpoint = torch.load(model_path, map_location = self.device)
+
+        if 'model' in checkpoint.keys():
+            self.model.load_state_dict(torch.load(model_path, map_location = self.device)['model'])
+        else:
+            self.model.load_state_dict(torch.load(model_path, map_location = self.device))
+
+        if self.cuda:
+            self.model = self.model.cuda()
+        self.model.eval()
+        self.transforms = transforms.Compose([
+            ToTensor()
+        ])
+
+        valid_scope = 2 * config.valid_scope + 1
+        self.anchors = util.generate_anchors(   config.total_stride,
+                                                config.anchor_base_size,
+                                                config.anchor_scales,
+                                                config.anchor_ratios,
+                                                valid_scope)
+        self.window = np.tile(np.outer(np.hanning(config.score_size), np.hanning(config.score_size))[None, :],
+                              [config.anchor_num, 1, 1]).flatten()
+
+        self.data_loader = TrackerDataLoader()
+
+
+    def _cosine_window(self, size):
+        """
+            get the cosine window
+        """
+        cos_window = np.hanning(int(size[0]))[:, np.newaxis].dot(np.hanning(int(size[1]))[np.newaxis, :])
+        cos_window = cos_window.astype(np.float32)
+        cos_window /= np.sum(cos_window)
+        return cos_window
+
+
+    def init(self, frame, bbox):
+
+        """ initialize siamfc tracker
+        Args:
+            frame: an RGB image
+            bbox: one-based bounding box [x, y, width, height]
+        """
+        frame = np.asarray(frame)
+        '''bbox[0] = bbox[0] + bbox[2]/2
+        bbox[1] = bbox[1] + bbox[3]/2'''
+
+        self.pos = np.array([bbox[0] + bbox[2] / 2 - 1 / 2, bbox[1] + bbox[3] / 2 - 1 / 2])  # center x, center y, zero based
+        self.target_sz = np.array([bbox[2], bbox[3]])  # width, height
+        self.bbox = np.array([bbox[0] + bbox[2] / 2 - 1 / 2, bbox[1] + bbox[3] / 2 - 1 / 2, bbox[2], bbox[3]])
+
+        self.origin_target_sz = np.array([bbox[2], bbox[3]])
+        self.img_mean = np.mean(frame, axis=(0, 1))
+
+        exemplar_img, _, _ = self.data_loader.get_exemplar_image(   frame,
+                                                                    self.bbox,
+                                                                    config.template_img_size,
+                                                                    config.context_amount,
+                                                                    self.img_mean)
+
+        # get exemplar feature
+        exemplar_img = self.transforms(exemplar_img)[None, :, :, :]
+        if self.cuda:
+            self.model.track_init(exemplar_img.cuda())
+        else:
+            self.model.track_init(exemplar_img)
+
+    def update(self, frame):
+        """track object based on the previous frame
+        Args:
+            frame: an RGB image
+
+        Returns:
+            bbox: tuple of 1-based bounding box(xmin, ymin, xmax, ymax)
+        """
+        frame = np.asarray(frame)
+
+        instance_img, _, _, scale_x = self.data_loader.get_instance_image(  frame,
+                                                                            self.bbox,
+                                                                            config.template_img_size,
+                                                                            config.detection_img_size,
+                                                                            config.context_amount,
+                                                                            self.img_mean)
+
+        instance_img = self.transforms(instance_img)[None, :, :, :]
+        if self.cuda:
+            pred_score, pred_regression = self.model.track(instance_img.cuda())
+        else:
+            pred_score, pred_regression = self.model.track(instance_img)
+
+        pred_conf   = pred_score.reshape(-1, 2, config.size ).permute(0, 2, 1)
+        pred_offset = pred_regression.reshape(-1, 4, config.size ).permute(0, 2, 1)
+
+        delta = pred_offset[0].cpu().detach().numpy()
+        box_pred = util.box_transform_inv(self.anchors, delta)
+        score_pred = F.softmax(pred_conf, dim=2)[0, :, 1].cpu().detach().numpy()
+
+        s_c = util.change(util.sz(box_pred[:, 2], box_pred[:, 3]) / (util.sz_wh(self.target_sz * scale_x)))  # scale penalty
+        r_c = util.change((self.target_sz[0] / self.target_sz[1]) / (box_pred[:, 2] / box_pred[:, 3]))  # ratio penalty
+        penalty = np.exp(-(r_c * s_c - 1.) * config.penalty_k)
+        pscore = penalty * score_pred
+        pscore = pscore * (1 - config.window_influence) + self.window * config.window_influence
+        best_pscore_id = np.argmax(pscore)
+        target = box_pred[best_pscore_id, :] / scale_x
+
+        lr = penalty[best_pscore_id] * score_pred[best_pscore_id] * config.lr_box
+
+        res_x = np.clip(target[0] + self.pos[0], 0, frame.shape[1])
+        res_y = np.clip(target[1] + self.pos[1], 0, frame.shape[0])
+
+        res_w = np.clip(self.target_sz[0] * (1 - lr) + target[2] * lr, config.min_scale * self.origin_target_sz[0],
+                        config.max_scale * self.origin_target_sz[0])
+        res_h = np.clip(self.target_sz[1] * (1 - lr) + target[3] * lr, config.min_scale * self.origin_target_sz[1],
+                        config.max_scale * self.origin_target_sz[1])
+
+        self.pos = np.array([res_x, res_y])
+        self.target_sz = np.array([res_w, res_h])
+
+        bbox = np.array([res_x, res_y, res_w, res_h])
+
+        self.bbox = (
+            np.clip(bbox[0], 0, frame.shape[1]).astype(np.float64),
+            np.clip(bbox[1], 0, frame.shape[0]).astype(np.float64),
+            np.clip(bbox[2], 10, frame.shape[1]).astype(np.float64),
+            np.clip(bbox[3], 10, frame.shape[0]).astype(np.float64))
+
+        res_x = res_x - res_w/2 # x -> x1
+        res_y = res_y - res_h/2 # y -> y1
+        bbox = np.array([res_x, res_y, res_w, res_h])
+
+        return bbox
diff --git a/tracking/data_loader.py b/tracking/data_loader.py
index 1a99c3c..45fe256 100644
--- a/tracking/data_loader.py
+++ b/tracking/data_loader.py
@@ -7,6 +7,7 @@ import random
 import numpy as np
 from torchvision import datasets, transforms, utils
 
+
 class TrackerDataLoader(object):
 
     def get_instance_image(self, img, bbox, size_z, size_x, context_amount, img_mean=None):
@@ -21,10 +22,7 @@ class TrackerDataLoader(object):
         instance_img, scale_x = self.crop_and_pad(img, cx, cy, size_x, s_x, img_mean)
         w_x = w * scale_x
         h_x = h * scale_x
-        # point_1 = (size_x + 1) / 2 - w_x / 2, (size_x + 1) / 2 - h_x / 2
-        # point_2 = (size_x + 1) / 2 + w_x / 2, (size_x + 1) / 2 + h_x / 2
-        # frame = cv2.rectangle(instance_img, (int(point_1[0]),int(point_1[1])), (int(point_2[0]),int(point_2[1])), (0, 255, 0), 2)
-        # cv2.imwrite('1.jpg', frame)
+
         return instance_img, w_x, h_x, scale_x
 
     def get_exemplar_image(self, img, bbox, size_z, context_amount, img_mean=None):
@@ -39,7 +37,6 @@ class TrackerDataLoader(object):
     def crop_and_pad(self, img, cx, cy, model_sz, original_sz, img_mean=None):
 
         def round_up(value):
-            # 替换内置round函数,实现保留2位小数的精确四舍五入
             return round(value + 1e-6 + 1000) - 1000
         im_h, im_w, _ = img.shape
 
@@ -59,7 +56,7 @@ class TrackerDataLoader(object):
         ymax = int(round_up(ymax + top))
         r, c, k = img.shape
         if any([top, bottom, left, right]):
-            te_im = np.zeros((r + top + bottom, c + left + right, k), np.uint8)  # 0 is better than 1 initialization
+            te_im = np.zeros((r + top + bottom, c + left + right, k), np.uint8)
             te_im[top:top + r, left:left + c, :] = img
             if top:
                 te_im[0:top, left:left + c, :] = img_mean
@@ -73,7 +70,7 @@ class TrackerDataLoader(object):
         else:
             im_patch_original = img[int(ymin):int(ymax + 1), int(xmin):int(xmax + 1), :]
         if not np.array_equal(model_sz, original_sz):
-            im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))  # zzp: use cv to get a better speed
+            im_patch = cv2.resize(im_patch_original, (model_sz, model_sz))
         else:
             im_patch = im_patch_original
         scale = model_sz / im_patch_original.shape[0]
diff --git a/tracking/network.py b/tracking/network.py
index 1704ab9..3c97a95 100644
--- a/tracking/network.py
+++ b/tracking/network.py
@@ -12,9 +12,178 @@ from torch import nn
 from config import config
 
 
+
+class _BatchNorm2d(nn.BatchNorm2d):
+
+    def __init__(self, num_features, *args, **kwargs):
+        super(_BatchNorm2d, self).__init__(
+            num_features, *args, eps=1e-6, momentum=0.05, **kwargs)
+
+
+class Dense(nn.Module):
+
+    def __init__(self, in_channels_rgb, in_channels_ir):
+        super(Dense, self).__init__()
+
+        #RGB BRANCH
+        self.conv1_rgb = nn.Sequential(
+            nn.Conv2d(in_channels_rgb, 96, 11, 2),
+            _BatchNorm2d(96),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1),
+            nn.MaxPool2d(3, 2))
+
+        self.conv2_rgb = nn.Sequential(
+            nn.Conv2d(96, 256, 5, 1, groups=2),
+            _BatchNorm2d(256),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1),
+            nn.MaxPool2d(3, 2))
+
+        self.conv3_rgb = nn.Sequential(
+            nn.Conv2d(256, 384, 3, 1),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1))
+
+        self.conv4_rgb = nn.Sequential(
+            nn.Conv2d(384, 384, 3, 1, groups=2),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1))
+
+        #IR BRANCH
+        self.conv1_ir = nn.Sequential(
+            nn.Conv2d(in_channels_ir, 96, 11, 2),
+            _BatchNorm2d(96),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(3, 2))
+
+        self.conv2_ir = nn.Sequential(
+            nn.Conv2d(96, 256, 5, 1, groups=2),
+            _BatchNorm2d(256),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(3, 2))
+
+        self.conv3_ir = nn.Sequential(
+            nn.Conv2d(256, 384, 3, 1),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True))
+        self.conv4_ir = nn.Sequential(
+            nn.Conv2d(384, 384, 3, 1, groups=2),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True))
+
+        # Shared layers
+        self.conv3_shared = nn.Sequential(
+            nn.Conv2d(512, 384, 5, 1, groups=2),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True))
+
+        self.conv5 = nn.Sequential(
+            nn.Conv2d(1152, 256, 3, 1, groups=2),
+            _BatchNorm2d(256))
+
+        # self.conv5 = nn.Sequential(
+        #     nn.Conv2d(1152, 512, 3, 1, groups=2),
+        #     _BatchNorm2d(512))
+
+
+    def forward(self, x_rgb, x_v):
+
+        # rgb branch
+        x_rgb = self.conv1_rgb(x_rgb)
+        x_rgb = self.conv2_rgb(x_rgb)
+        # ir branch
+        x_v = self.conv1_ir(x_v)
+        x_v = self.conv2_ir(x_v)
+
+        x_shared = torch.cat((x_rgb, x_v), 1)
+        x_shared = self.conv3_shared(x_shared)
+
+        x_rgb = self.conv3_rgb(x_rgb)
+        x_rgb = self.conv4_rgb(x_rgb)
+
+
+        x_v = self.conv3_ir(x_v)
+        x_v = self.conv4_ir(x_v)
+
+        x_final = torch.cat((x_rgb, x_shared, x_v), 1)
+        x_final = self.conv5(x_final)
+
+        return x_final
+
+
+
+
+
+
+class AlexNetV3MM(nn.Module):
+    output_stride = 8
+
+
+    def __init__(self):
+
+        super(AlexNetV3MM, self).__init__()
+
+        self.conv1 = nn.Sequential(
+            nn.Conv2d(3+1, 192, 11, 2),
+            _BatchNorm2d(192),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(3, 2))
+        self.conv2 = nn.Sequential(
+            nn.Conv2d(192, 512, 5, 1),
+            _BatchNorm2d(512),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(3, 2))
+
+        self.conv3 = nn.Sequential(
+            nn.Conv2d(512, 768, 3, 1),
+            _BatchNorm2d(768),
+            nn.ReLU(inplace=True))
+
+        self.conv4 = nn.Sequential(
+            nn.Conv2d(768, 768, 3, 1),
+            _BatchNorm2d(768),
+            nn.ReLU(inplace=True))
+
+        self.conv5 = nn.Sequential(
+            nn.Conv2d(768, 512, 3, 1),
+            _BatchNorm2d(512))
+
+
+    def forward(self, x_rgb, x_ir):
+        x = torch.cat((x_rgb, x_ir), 1)
+
+        x = self.conv1(x)
+        x = self.conv2(x)
+        x = self.conv3(x)
+        x = self.conv4(x)
+        x = self.conv5(x)
+        return x
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
+
 class SiameseAlexNet(nn.Module):
     def __init__(self, ):
         super(SiameseAlexNet, self).__init__()
+
+
         self.featureExtract = nn.Sequential(
             nn.Conv2d(3, 96, 11, stride=2),
             nn.BatchNorm2d(96),
@@ -33,6 +202,8 @@ class SiameseAlexNet(nn.Module):
             nn.Conv2d(384, 256, 3),
             nn.BatchNorm2d(256),
         )
+
+
         self.anchor_num = config.anchor_num
         self.input_size = config.detection_img_size
         self.score_displacement = int((self.input_size - config.template_img_size) / config.total_stride)
@@ -54,10 +225,95 @@ class SiameseAlexNet(nn.Module):
                 m.bias.data.zero_()
 
     def forward(self, template, detection):
+
+
+        N = template.size(0)
+
+
+        template_feature = self.featureExtract(template)
+        detection_feature = self.featureExtract(detection)
+
+        kernel_score = self.conv_cls1(template_feature).view(N, 2 * self.anchor_num, 256, 4, 4)
+        kernel_regression = self.conv_r1(template_feature).view(N, 4 * self.anchor_num, 256, 4, 4)
+        conv_score = self.conv_cls2(detection_feature)
+        conv_regression = self.conv_r2(detection_feature)
+
+        conv_scores = conv_score.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        score_filters = kernel_score.reshape(-1, 256, 4, 4)
+        pred_score = F.conv2d(conv_scores, score_filters, groups=N).reshape(N, 10, self.score_displacement + 1,
+                                                                            self.score_displacement + 1)
+
+        conv_reg = conv_regression.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        reg_filters = kernel_regression.reshape(-1, 256, 4, 4)
+        pred_regression = self.regress_adjust(
+            F.conv2d(conv_reg, reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,
+                                                              self.score_displacement + 1))
+        return pred_score, pred_regression
+
+    def track_init(self, template):
         N = template.size(0)
         template_feature = self.featureExtract(template)
+
+        kernel_score = self.conv_cls1(template_feature).view(N, 2 * self.anchor_num, 256, 4, 4)
+        kernel_regression = self.conv_r1(template_feature).view(N, 4 * self.anchor_num, 256, 4, 4)
+        self.score_filters = kernel_score.reshape(-1, 256, 4, 4)
+        self.reg_filters = kernel_regression.reshape(-1, 256, 4, 4)
+
+    def track(self, detection):
+        N = detection.size(0)
         detection_feature = self.featureExtract(detection)
 
+        conv_score = self.conv_cls2(detection_feature)
+        conv_regression = self.conv_r2(detection_feature)
+
+        conv_scores = conv_score.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        pred_score = F.conv2d(conv_scores, self.score_filters, groups=N).reshape(N, 10, self.score_displacement + 1,
+                                                                                 self.score_displacement + 1)
+        conv_reg = conv_regression.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        pred_regression = self.regress_adjust(
+            F.conv2d(conv_reg, self.reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,
+                                                                   self.score_displacement + 1))
+        return pred_score, pred_regression
+
+
+class SiameseAlexNetMultimodal(nn.Module):
+
+
+    def __init__(self, ):
+        super(SiameseAlexNetMultimodal, self).__init__()
+
+        self.featureExtract = Dense(3, 1)
+
+        self.anchor_num = config.anchor_num
+        self.input_size = config.detection_img_size
+        self.score_displacement = int((self.input_size - config.template_img_size) / config.total_stride)
+
+        self.conv_cls1 = nn.Conv2d(256, 256 * 2 * self.anchor_num, kernel_size=3, stride=1, padding=0)
+        self.conv_r1 = nn.Conv2d(256, 256 * 4 * self.anchor_num, kernel_size=3, stride=1, padding=0)
+
+        self.conv_cls2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0)
+        self.conv_r2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0)
+        self.regress_adjust = nn.Conv2d(4 * self.anchor_num, 4 * self.anchor_num, 1)
+
+
+    def init_weights(self):
+        for m in self.modules():
+            if isinstance(m, nn.Conv2d):
+                # nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')
+                nn.init.normal_(m.weight.data, std=0.0005)
+                nn.init.normal_(m.bias.data, std=0.0005)
+            elif isinstance(m, nn.BatchNorm2d):
+                m.weight.data.fill_(1)
+                m.bias.data.zero_()
+
+    def forward(self, template_rgb, detection_rgb, template_ir, detection_ir):
+
+        N = template_rgb.size(0)
+
+        template_feature = self.featureExtract(template_rgb, template_ir)
+        detection_feature = self.featureExtract(detection_rgb, detection_ir)
+
+
         kernel_score = self.conv_cls1(template_feature).view(N, 2 * self.anchor_num, 256, 4, 4)
         kernel_regression = self.conv_r1(template_feature).view(N, 4 * self.anchor_num, 256, 4, 4)
         conv_score = self.conv_cls2(detection_feature)
@@ -86,6 +342,7 @@ class SiameseAlexNet(nn.Module):
 
     def track(self, detection):
         N = detection.size(0)
+
         detection_feature = self.featureExtract(detection)
 
         conv_score = self.conv_cls2(detection_feature)
@@ -99,3 +356,4 @@ class SiameseAlexNet(nn.Module):
             F.conv2d(conv_reg, self.reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,
                                                                    self.score_displacement + 1))
         return pred_score, pred_regression
+
diff --git a/tracking/run_tracking.py b/tracking/run_tracking.py
index 68b9ed1..5961b82 100644
--- a/tracking/run_tracking.py
+++ b/tracking/run_tracking.py
@@ -1,22 +1,25 @@
 from __future__ import absolute_import
 from got10k.experiments import *
-from siamRPNBIG import TrackerSiamRPNBIG
+from .SiamRPNEval import TrackerSiamRPNEval
 import argparse
 import os
 import json
+from train.experimentrgbt import ExperimentRGBT
+
 
-parser = argparse.ArgumentParser(description='PyTorch SiameseRPN Tracking')
 
-parser.add_argument('--tracker_path', default='/home/arbi/desktop/data', metavar='DIR',help='path to dataset')
+parser = argparse.ArgumentParser(description='PyTorch SiameseRPN Tracking')
 parser.add_argument('--experiment_name', default='default', metavar='DIR',help='path to weight')
-parser.add_argument('--net_path', default='../train/experiments/default/model/model_e17.pth', metavar='DIR',help='path to weight')
-# ../train/experiments/default/model/model_e1.pth # ../model.pth #../siamrpn_7.pth
-# /Users/arbi/Desktop # /home/arbi/desktop/GOT-10k
-# /media/arbi/9132EE0B9756C987/dataset/GOT-10k/full_data
-parser.add_argument('--visualize', default=True, help='visualize')
+
+parser.add_argument('--net_path', default='../train/experiments/SiamRPN_RGB/model/model_e2.pth', metavar='DIR',help='path to weight')
+# parser.add_argument('--net_path', default='../train/experiments/SiamRPN_RGBIR/model/model_e2.pth', metavar='DIR',help='path to weight')
+
+parser.add_argument('--visualize', default=False, help='visualize')
 
 args = parser.parse_args()
 
+
+
 if __name__ == '__main__':
 
     """Load the parameters from json file"""
@@ -26,7 +29,11 @@ if __name__ == '__main__':
         params = json.load(data_file)
 
     '''setup tracker'''
-    tracker = TrackerSiamRPNBIG(params, args.net_path)
+
+
+    # tracker = TrackerSiamRPNEval(params, modality=1, model_path=args.net_path)
+    tracker = TrackerSiamRPNEval(params, modality=2, model_path=args.net_path)
+
 
     '''setup experiments'''
     # 7 datasets with different versions
@@ -54,11 +61,19 @@ if __name__ == '__main__':
                     report_dir='experiments/{}/reports'.format(args.experiment_name))
 
     '''
-    experiments = ExperimentOTB('/home/arbi/desktop/data', version=2015,
-                    result_dir='experiments/{}/OTB2015resultsGOT-10k_42'.format(args.experiment_name),
-                    report_dir='experiments/{}/OTB2015reportsGOT-10k_42'.format(args.experiment_name))
+    # experiments = ExperimentOTB('/home/arbi/desktop/data', version=2015,
+    #                 result_dir='experiments/{}/OTB2015resultsGOT-10k_42'.format(args.experiment_name),
+    #                 report_dir='experiments/{}/OTB2015reportsGOT-10k_42'.format(args.experiment_name))
+
+    # experiments = ExperimentGOT10k(args.tracker_path, subset='val',
+    #                 result_dir='experiments/{}/results'.format(args.experiment_name),
+    #                 report_dir='experiments/{}/reports'.format(args.experiment_name))
+
+    experiments = ExperimentRGBT('/home/zuern/datasets/thermal_tracking/RGB-T234/',
+                       experiment_name='RGBT-Train',
+                       subset='val')
 
 
     '''run experiments'''
     experiments.run(tracker, visualize = args.visualize)
-    experiments.report([tracker.name])
+    experiments.report([tracker.name], return_report=False)
diff --git a/tracking/siamRPNBIG.py b/tracking/siamRPNBIG.py
deleted file mode 100644
index c304b83..0000000
--- a/tracking/siamRPNBIG.py
+++ /dev/null
@@ -1,219 +0,0 @@
-import cv2
-import torch
-import numpy as np
-import torch.nn as nn
-from util import util
-import torch.nn.functional as F
-from config import TrackerConfig
-import torchvision.transforms as transforms
-from custom_transforms import ToTensor
-from config import config
-from torch.autograd import Variable
-from got10k.trackers import Tracker
-from network import SiameseAlexNet
-from data_loader import TrackerDataLoader
-from PIL import Image, ImageOps, ImageStat, ImageDraw
-
-class SiamRPN(nn.Module):
-
-    def __init__(self, anchor_num = 5):
-        super(SiamRPN, self).__init__()
-
-        self.anchor_num = anchor_num
-        self.feature = nn.Sequential(
-            # conv1
-            nn.Conv2d(3, 64, kernel_size = 11, stride = 2),
-            nn.BatchNorm2d(64),
-            nn.ReLU(inplace = True),
-            nn.MaxPool2d(kernel_size = 3, stride = 2),
-            # conv2
-            nn.Conv2d(64, 192, kernel_size = 5),
-            nn.BatchNorm2d(192),
-            nn.ReLU(inplace=True),
-            nn.MaxPool2d(kernel_size = 3, stride = 2),
-            # conv3
-            nn.Conv2d(192, 384, kernel_size = 3),
-            nn.BatchNorm2d(384),
-            nn.ReLU(inplace = True),
-            # conv4
-            nn.Conv2d(384, 256, kernel_size = 3),
-            nn.BatchNorm2d(256),
-            nn.ReLU(inplace = True),
-            # conv5
-            nn.Conv2d(256, 256, kernel_size = 3),
-            nn.BatchNorm2d(256))
-
-        self.conv_reg_z = nn.Conv2d(256, 256 * 4 * self.anchor_num, 3, 1)
-        self.conv_reg_x = nn.Conv2d(256, 256, 3)
-        self.conv_cls_z = nn.Conv2d(256, 256 * 2 * anchor_num, 3, 1)
-        self.conv_cls_x = nn.Conv2d(256, 256, 3)
-        self.adjust_reg = nn.Conv2d(4 * anchor_num, 4 * anchor_num*1, 1)
-
-    def forward(self, z, x):
-        return self.inference(x, *self.learn(z))
-
-    def learn(self, z):
-        z = self.feature(z)
-        kernel_reg = self.conv_reg_z(z)
-        kernel_cls = self.conv_cls_z(z)
-
-        k = kernel_reg.size()[-1]
-        kernel_reg = kernel_reg.view(4 * self.anchor_num, 256, k, k)
-        kernel_cls = kernel_cls.view(2 * self.anchor_num, 256, k, k)
-
-        return kernel_reg, kernel_cls
-
-    def inference(self, x, kernel_reg, kernel_cls):
-        x = self.feature(x)
-        x_reg = self.conv_reg_x(x)
-        x_cls = self.conv_cls_x(x)
-
-        out_reg = self.adjust_reg(F.conv2d(x_reg, kernel_reg))
-        out_cls = F.conv2d(x_cls, kernel_cls)
-
-        return out_reg, out_cls
-
-class TrackerSiamRPNBIG(Tracker):
-    def __init__(self, params, model_path = None, **kargs):
-        super(TrackerSiamRPNBIG, self).__init__(name='SiamRPN', is_deterministic=True)
-
-        self.model = SiameseAlexNet()
-
-        self.cuda = torch.cuda.is_available()
-        self.device = torch.device('cuda:0' if self.cuda else 'cpu')
-
-        checkpoint = torch.load(model_path, map_location = self.device)
-        #print("1")
-        if 'model' in checkpoint.keys():
-            self.model.load_state_dict(torch.load(model_path, map_location = self.device)['model'])
-        else:
-            self.model.load_state_dict(torch.load(model_path, map_location = self.device))
-
-
-        if self.cuda:
-            self.model = self.model.cuda()
-        self.model.eval()
-        self.transforms = transforms.Compose([
-            ToTensor()
-        ])
-
-        valid_scope = 2 * config.valid_scope + 1
-        self.anchors = util.generate_anchors(   config.total_stride,
-                                                config.anchor_base_size,
-                                                config.anchor_scales,
-                                                config.anchor_ratios,
-                                                valid_scope)
-        self.window = np.tile(np.outer(np.hanning(config.score_size), np.hanning(config.score_size))[None, :],
-                              [config.anchor_num, 1, 1]).flatten()
-
-        self.data_loader = TrackerDataLoader()
-
-    def _cosine_window(self, size):
-        """
-            get the cosine window
-        """
-        cos_window = np.hanning(int(size[0]))[:, np.newaxis].dot(np.hanning(int(size[1]))[np.newaxis, :])
-        cos_window = cos_window.astype(np.float32)
-        cos_window /= np.sum(cos_window)
-        return cos_window
-
-    def init(self, frame, bbox):
-
-        """ initialize siamfc tracker
-        Args:
-            frame: an RGB image
-            bbox: one-based bounding box [x, y, width, height]
-        """
-        frame = np.asarray(frame)
-        '''bbox[0] = bbox[0] + bbox[2]/2
-        bbox[1] = bbox[1] + bbox[3]/2'''
-
-        self.pos = np.array([bbox[0] + bbox[2] / 2 - 1 / 2, bbox[1] + bbox[3] / 2 - 1 / 2])  # center x, center y, zero based
-        #self.pos = np.array([bbox[0], bbox[1]])  # center x, center y, zero based
-
-        self.target_sz = np.array([bbox[2], bbox[3]])  # width, height
-        self.bbox = np.array([bbox[0] + bbox[2] / 2 - 1 / 2, bbox[1] + bbox[3] / 2 - 1 / 2, bbox[2], bbox[3]])
-        #self.bbox = np.array([bbox[0], bbox[1], bbox[2], bbox[3]])
-
-        self.origin_target_sz = np.array([bbox[2], bbox[3]])
-        # get exemplar img
-        self.img_mean = np.mean(frame, axis=(0, 1))
-
-        exemplar_img, _, _ = self.data_loader.get_exemplar_image(   frame,
-                                                                    self.bbox,
-                                                                    config.template_img_size,
-                                                                    config.context_amount,
-                                                                    self.img_mean)
-
-        #cv2.imshow('exemplar_img', exemplar_img)
-        # get exemplar feature
-        exemplar_img = self.transforms(exemplar_img)[None, :, :, :]
-        if self.cuda:
-            self.model.track_init(exemplar_img.cuda())
-        else:
-            self.model.track_init(exemplar_img)
-
-    def update(self, frame):
-        """track object based on the previous frame
-        Args:
-            frame: an RGB image
-
-        Returns:
-            bbox: tuple of 1-based bounding box(xmin, ymin, xmax, ymax)
-        """
-        frame = np.asarray(frame)
-
-        instance_img, _, _, scale_x = self.data_loader.get_instance_image(  frame,
-                                                                            self.bbox,
-                                                                            config.template_img_size,
-                                                                            config.detection_img_size,
-                                                                            config.context_amount,
-                                                                            self.img_mean)
-        #cv2.imshow('instance_img', instance_img)
-
-        instance_img = self.transforms(instance_img)[None, :, :, :]
-        if self.cuda:
-            pred_score, pred_regression = self.model.track(instance_img.cuda())
-        else:
-            pred_score, pred_regression = self.model.track(instance_img)
-
-        pred_conf   = pred_score.reshape(-1, 2, config.size ).permute(0, 2, 1)
-        pred_offset = pred_regression.reshape(-1, 4, config.size ).permute(0, 2, 1)
-
-        delta = pred_offset[0].cpu().detach().numpy()
-        box_pred = util.box_transform_inv(self.anchors, delta)
-        score_pred = F.softmax(pred_conf, dim=2)[0, :, 1].cpu().detach().numpy()
-
-        s_c = util.change(util.sz(box_pred[:, 2], box_pred[:, 3]) / (util.sz_wh(self.target_sz * scale_x)))  # scale penalty
-        r_c = util.change((self.target_sz[0] / self.target_sz[1]) / (box_pred[:, 2] / box_pred[:, 3]))  # ratio penalty
-        penalty = np.exp(-(r_c * s_c - 1.) * config.penalty_k)
-        pscore = penalty * score_pred
-        pscore = pscore * (1 - config.window_influence) + self.window * config.window_influence
-        best_pscore_id = np.argmax(pscore)
-        target = box_pred[best_pscore_id, :] / scale_x
-
-        lr = penalty[best_pscore_id] * score_pred[best_pscore_id] * config.lr_box
-
-        res_x = np.clip(target[0] + self.pos[0], 0, frame.shape[1])
-        res_y = np.clip(target[1] + self.pos[1], 0, frame.shape[0])
-
-        res_w = np.clip(self.target_sz[0] * (1 - lr) + target[2] * lr, config.min_scale * self.origin_target_sz[0],
-                        config.max_scale * self.origin_target_sz[0])
-        res_h = np.clip(self.target_sz[1] * (1 - lr) + target[3] * lr, config.min_scale * self.origin_target_sz[1],
-                        config.max_scale * self.origin_target_sz[1])
-
-        self.pos = np.array([res_x, res_y])
-        self.target_sz = np.array([res_w, res_h])
-
-        bbox = np.array([res_x, res_y, res_w, res_h])
-        #print('bbox', bbox)
-        self.bbox = (
-            np.clip(bbox[0], 0, frame.shape[1]).astype(np.float64),
-            np.clip(bbox[1], 0, frame.shape[0]).astype(np.float64),
-            np.clip(bbox[2], 10, frame.shape[1]).astype(np.float64),
-            np.clip(bbox[3], 10, frame.shape[0]).astype(np.float64))
-
-        res_x = res_x - res_w/2 # x -> x1
-        res_y = res_y - res_h/2 # y -> y1
-        bbox = np.array([res_x, res_y, res_w, res_h])
-        return bbox
diff --git a/train/config.py b/train/config.py
index 76c65f1..b0195ef 100644
--- a/train/config.py
+++ b/train/config.py
@@ -37,7 +37,8 @@ class Config(object):
     out_feature = 19
     max_inter   = 80
     fix_former_3_layers = True
-    pretrained_model =  '/home/arbi/Загрузки/alexnet.pth' # '/home/arbi/desktop/alexnet.pth' # # '/Users/arbi/Desktop/alexnet.pth'
+    # pretrained_model =  '/home/arbi/Загрузки/alexnet.pth' # '/home/arbi/desktop/alexnet.pth' # # '/Users/arbi/Desktop/alexnet.pth'
+    pretrained_model =  None
 
     total_stride = 8
     anchor_total_stride = total_stride
diff --git a/train/custom_transforms.py b/train/custom_transforms.py
index 557a2fb..b6657ae 100644
--- a/train/custom_transforms.py
+++ b/train/custom_transforms.py
@@ -158,5 +158,10 @@ class Normalize(object):
 
 class ToTensor(object):
     def __call__(self, sample):
-        sample = sample.transpose(2, 0, 1)
+
+        if len(sample.shape) > 2:
+            sample = sample.transpose(2, 0, 1)
+        else:
+            sample = np.expand_dims(sample, axis=0)
+
         return torch.from_numpy(sample.astype(np.float32))
diff --git a/train/experiments/default/parameters.json b/train/experiments/default/parameters.json
deleted file mode 100644
index 2a0a6ff..0000000
--- a/train/experiments/default/parameters.json
+++ /dev/null
@@ -1,14 +0,0 @@
-{
-    "template_img_size": 127,
-    "detection_img_size": 271,
-    "stride": 8,
-    "lr": 1e-5,
-    "epoches": 200,
-    "weight_decay": 0.0005,
-    "momentum": 0.9,
-    "context": 0.5,
-    "ratios": [0.33, 0.5, 1, 2, 3],
-    "scales": [8],
-    "penalty_k": 0.055,
-    "window_influence": 0.42
-}
diff --git a/train/net.py b/train/net.py
index 611ae89..6be1bfc 100644
--- a/train/net.py
+++ b/train/net.py
@@ -1,32 +1,32 @@
 # -*- coding: utf-8 -*-
 
 import os
-import cv2
 import torch
-import random
-import numpy as np
-import torch.nn as nn
 from util import util
-import matplotlib.pyplot as plt
-import torch.nn.functional as F
 from config import config
 from got10k.trackers import Tracker
-from network import SiameseAlexNet
+from tracking.network import SiameseAlexNet, SiameseAlexNetMultimodal
+from network_siamfc import Net, SiamFC, AlexNetV3
 from loss import rpn_smoothL1, rpn_cross_entropy_balance
 
+
 class TrackerSiamRPN(Tracker):
 
-    def __init__(self, net_path=None, **kargs):
+    def __init__(self, net_path=None, modality=1, **kargs):
         super(TrackerSiamRPN, self).__init__(
             name='SiamRPN', is_deterministic=True)
 
         '''setup GPU device if available'''
         self.cuda   = torch.cuda.is_available()
         self.device = torch.device('cuda:0' if self.cuda else 'cpu')
+        self.modality = modality
 
         '''setup model'''
-        self.net = SiameseAlexNet()
-        #self.net.init_weights()
+        if self.modality == 1:
+            self.net = SiameseAlexNet()
+        else:
+            self.net = SiameseAlexNetMultimodal()
+
 
         if net_path is not None:
             self.net.load_state_dict(torch.load(
@@ -41,6 +41,7 @@ class TrackerSiamRPN(Tracker):
             momentum     = config.momentum,
             weight_decay = config.weight_decay)
 
+
     def step(self, epoch, dataset, anchors, i = 0,  train=True):
 
         if train:
@@ -48,13 +49,20 @@ class TrackerSiamRPN(Tracker):
         else:
             self.net.eval()
 
-        template, detection, regression_target, conf_target = dataset
+
+        template_rgb, detection_rgb, template_ir, detection_ir, regression_target, conf_target = dataset
 
         if self.cuda:
-            template, detection = template.cuda(), detection.cuda()
+            template_rgb, detection_rgb = template_rgb.cuda(), detection_rgb.cuda()
+            template_ir, detection_ir = template_ir.cuda(), detection_ir.cuda()
             regression_target, conf_target = regression_target.cuda(), conf_target.cuda()
 
-        pred_score, pred_regression = self.net(template, detection)
+
+        if self.modality == 1:
+            pred_score, pred_regression = self.net(template_rgb, detection_rgb)
+        else:
+            pred_score, pred_regression = self.net(template_rgb, detection_rgb, template_ir, detection_ir)
+
 
         pred_conf   = pred_score.reshape(-1, 2, config.size).permute(0, 2, 1)
 
@@ -76,30 +84,6 @@ class TrackerSiamRPN(Tracker):
 
         loss = cls_loss + config.lamb * reg_loss
 
-        '''anchors_show = anchors
-        exem_img = template[0].cpu().numpy().transpose(1, 2, 0)  # (127, 127, 3)
-        #cv2.imwrite('exem_img.png', exem_img)
-
-        inst_img = detection[0].cpu().numpy().transpose(1, 2, 0) # (255, 255, 3)
-        #cv2.imwrite('inst_img.png', inst_img)
-
-
-
-        topk = 1
-        cls_pred = F.softmax(pred_conf, dim=2)[0, :, 1]
-
-        topk_box = util.get_topk_box(cls_pred, pred_offset[0], anchors_show, topk=topk)
-        img_box = util.add_box_img(inst_img, topk_box, color=(0, 0, 255))
-
-        cv2.imwrite('pred_inst.png', img_box)
-
-        cls_pred = conf_target[0]
-        gt_box = util.get_topk_box(cls_pred, regression_target[0], anchors_show)
-        #print('gt_box', gt_box)
-        img_box = util.add_box_img(img_box, gt_box, color=(255, 0, 0), x = 1, y = 1)
-        #print('gt_box', gt_box)
-        cv2.imwrite('pred_inst_gt.png', img_box)'''
-
         if train:
             self.optimizer.zero_grad()
             loss.backward()
@@ -118,61 +102,76 @@ class TrackerSiamRPN(Tracker):
         net_path = os.path.join(model_save_dir_pth, 'model_e%d.pth' % (epoch + 1))
         torch.save(model.net.state_dict(), net_path)
 
-'''class SiamRPN(nn.Module):
-
-    def __init__(self, anchor_num = 5):
-        super(SiamRPN, self).__init__()
-
-        self.anchor_num = anchor_num
-        self.feature = nn.Sequential(
-            # conv1
-            nn.Conv2d(3, 64, kernel_size = 11, stride = 2),
-            nn.BatchNorm2d(64),
-            nn.ReLU(inplace = True),
-            nn.MaxPool2d(kernel_size = 3, stride = 2),
-            # conv2
-            nn.Conv2d(64, 192, kernel_size = 5),
-            nn.BatchNorm2d(192),
-            nn.ReLU(inplace=True),
-            nn.MaxPool2d(kernel_size = 3, stride = 2),
-            # conv3
-            nn.Conv2d(192, 384, kernel_size = 3),
-            nn.BatchNorm2d(384),
-            nn.ReLU(inplace = True),
-            # conv4
-            nn.Conv2d(384, 256, kernel_size = 3),
-            nn.BatchNorm2d(256),
-            nn.ReLU(inplace = True),
-            # conv5
-            nn.Conv2d(256, 256, kernel_size = 3),
-            nn.BatchNorm2d(256))
-
-        self.conv_reg_z = nn.Conv2d(256, 256 * 4 * self.anchor_num, 3, 1)
-        self.conv_reg_x = nn.Conv2d(256, 256, 3)
-        self.conv_cls_z = nn.Conv2d(256, 256 * 2 * anchor_num, 3, 1)
-        self.conv_cls_x = nn.Conv2d(256, 256, 3)
-        self.adjust_reg = nn.Conv2d(4 * anchor_num, 4 * anchor_num*1, 1)
-
-    def forward(self, z, x):
-        return self.inference(x, *self.learn(z))
-
-    def learn(self, z):
-        z = self.feature(z)
-        kernel_reg = self.conv_reg_z(z)
-        kernel_cls = self.conv_cls_z(z)
-
-        k = kernel_reg.size()[-1]
-        kernel_reg = kernel_reg.view(4 * self.anchor_num, 256, k, k)
-        kernel_cls = kernel_cls.view(2 * self.anchor_num, 256, k, k)
-
-        return kernel_reg, kernel_cls
-
-    def inference(self, x, kernel_reg, kernel_cls):
-        x = self.feature(x)
-        x_reg = self.conv_reg_x(x)
-        x_cls = self.conv_cls_x(x)
-
-        out_reg = self.adjust_reg(F.conv2d(x_reg, kernel_reg))
-        out_cls = F.conv2d(x_cls, kernel_cls)
-
-        return out_reg, out_cls'''
+
+class TrackerSiamFC(Tracker):
+
+    def __init__(self, net_path=None, **kwargs):
+
+        super(TrackerSiamFC, self).__init__('SiamFC', True)
+
+        # setup GPU device if available
+        self.cuda = torch.cuda.is_available()
+        self.device = torch.device('cuda:0' if self.cuda else 'cpu')
+
+        print('Training on device {}'.format(self.device))
+
+        '''setup GPU device if available'''
+        self.cuda = torch.cuda.is_available()
+        self.device = torch.device('cuda:0' if self.cuda else 'cpu')
+
+
+
+        # setup model
+        self.net = Net(
+            backbone=AlexNetV3(),
+            head=SiamFC(self.cfg.out_scale))
+
+        # load checkpoint if provided
+        if net_path is not None:
+            self.net.load_state_dict(torch.load(
+                net_path, map_location=lambda storage, loc: storage))
+        self.net = self.net.to(self.device)
+
+
+        # setup criterion
+        from network_siamfc import BalancedLoss
+        self.criterion = BalancedLoss()
+
+        # setup optimizer
+        self.optimizer = torch.optim.SGD(
+            self.net.parameters(),
+            lr=self.cfg.initial_lr,
+            weight_decay=self.cfg.weight_decay,
+            momentum=self.cfg.momentum)
+
+
+    def step(self, epoch, dataset, anchors, i = 0,  train=True):
+
+        # TODO: implement further
+
+        if train:
+            self.net.train()
+        else:
+            self.net.eval()
+
+        template, detection, regression_target, conf_target = dataset
+
+        if self.cuda:
+            template, detection = template.cuda(), detection.cuda()
+            regression_target, conf_target = regression_target.cuda(), conf_target.cuda()
+
+
+
+        responses = self.net(template, detection)
+        loss = self.criterion(responses, conf_target)
+
+        if train:
+            self.optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_norm_(self.net.parameters(), config.clip)
+            self.optimizer.step()
+
+        return loss
+
+
+
diff --git a/train/train_siamrpn.py b/train/train_siamrpn.py
index 42c0ffd..99d380b 100644
--- a/train/train_siamrpn.py
+++ b/train/train_siamrpn.py
@@ -10,50 +10,86 @@ import numpy as np
 from tqdm import tqdm
 from torch.nn import init
 from config import config
-from net import TrackerSiamRPN
-from data import TrainDataLoader
+from net import TrackerSiamRPN, TrackerSiamFC
+from data_rgbt import TrainDataLoaderRGBT
 from torch.utils.data import DataLoader
+
 from util import util, AverageMeter, SavePlot
 from got10k.datasets import ImageNetVID, GOT10k
 from torchvision import datasets, transforms, utils
 from custom_transforms import Normalize, ToTensor, RandomStretch, \
     RandomCrop, CenterCrop, RandomBlur, ColorAug
+import wandb
+import shutil
+
+from train.experimentrgbt import RGBTSequence
+
 
 torch.manual_seed(1234) # config.seed
 
 
 parser = argparse.ArgumentParser(description='PyTorch SiameseRPN Training')
 
-parser.add_argument('--train_path', default='/home/arbi/desktop/GOT-10k', metavar='DIR',help='path to dataset')
-parser.add_argument('--experiment_name', default='default', metavar='DIR',help='path to weight')
+parser.add_argument('--train_path', default='/home/zuern/datasets/tracking/GOT10k', metavar='DIR',help='path to dataset')
+parser.add_argument('--experiment_name', default='SiamRPN', metavar='DIR',help='path to weight')
 parser.add_argument('--checkpoint_path', default=None, help='resume')
-# /home/arbi/desktop/GOT-10k # /Users/arbi/Desktop # /home/arbi/desktop/ILSVRC
-# 'experiments/default/model/model_e1.pth'
+parser.add_argument('--modality', default=None, type=int, help='how many modalities')
+parser.add_argument('--model', default=None, type=str, help='which model to use', choices=['SiamFC', 'SiamRPN'])
+
+
 def main():
 
     '''parameter initialization'''
     args = parser.parse_args()
     exp_name_dir = util.experiment_name_dir(args.experiment_name)
 
+
+    wandb.init(project="SiameseX", reinit=True)
+    wandb.config.update(args)  # adds all of the arguments as config variables
+
+
     '''model on gpu'''
-    model = TrackerSiamRPN()
+    if args.model == 'SiamRPN':
+        model = TrackerSiamRPN(modality=args.modality)
+    elif args.model == 'SiamFC':
+        model = TrackerSiamFC(modality=args.modality)
+    else:
+        raise ValueError('Unknown model')
+
+    name = 'RGBT-234'
+
+    assert name in ['VID', 'GOT-10k', 'All', 'RGBT-234']
 
-    '''setup train data loader'''
-    name = 'GOT-10k'
-    assert name in ['VID', 'GOT-10k', 'All']
     if name == 'GOT-10k':
         root_dir = args.train_path
         seq_dataset = GOT10k(root_dir, subset='train')
+        seq_dataset_val = GOT10k(root_dir, subset='val')
+
     elif name == 'VID':
-        root_dir = '/home/arbi/desktop/ILSVRC'
+        root_dir = '/home/zuern/datasets/tracking/VID/ILSVRC'
         seq_dataset = ImageNetVID(root_dir, subset=('train'))
+        seq_dataset_val = ImageNetVID(root_dir, subset=('val'))
+
+    elif name == 'RGBT-234':
+        seq_dataset = RGBTSequence('/home/zuern/datasets/thermal_tracking/RGB-T234/', subset='train')
+        seq_dataset_val = RGBTSequence('/home/zuern/datasets/thermal_tracking/RGB-T234/', subset='val')
+
+
     elif name == 'All':
         root_dir_vid = '/home/arbi/desktop/ILSVRC'
         seq_datasetVID = ImageNetVID(root_dir_vid, subset=('train'))
         root_dir_got = args.train_path
         seq_datasetGOT = GOT10k(root_dir_got, subset='train')
         seq_dataset = util.data_split(seq_datasetVID, seq_datasetGOT)
+
+        seq_datasetVID = ImageNetVID(root_dir_vid, subset=('val'))
+        root_dir_got = args.train_path
+        seq_datasetGOT = GOT10k(root_dir_got, subset='val')
+        seq_dataset_val = util.data_split(seq_datasetVID, seq_datasetGOT)
+
     print('seq_dataset', len(seq_dataset))
+    print('seq_dataset_val', len(seq_dataset_val))
+
 
     train_z_transforms = transforms.Compose([
         ToTensor()
@@ -62,7 +98,14 @@ def main():
         ToTensor()
     ])
 
-    train_data  = TrainDataLoader(seq_dataset, train_z_transforms, train_x_transforms, name)
+    val_z_transforms = transforms.Compose([
+        ToTensor()
+    ])
+    val_x_transforms = transforms.Compose([
+        ToTensor()
+    ])
+
+    train_data  = TrainDataLoaderRGBT(seq_dataset, train_z_transforms, train_x_transforms, name)
     anchors = train_data.anchors
     train_loader = DataLoader(  dataset    = train_data,
                                 batch_size = config.train_batch_size,
@@ -70,31 +113,7 @@ def main():
                                 num_workers= config.train_num_workers,
                                 pin_memory = True)
 
-    '''setup val data loader'''
-    name = 'GOT-10k'
-    assert name in ['VID', 'GOT-10k', 'All']
-    if name == 'GOT-10k':
-        root_dir = args.train_path
-        seq_dataset_val = GOT10k(root_dir, subset='val')
-    elif name == 'VID':
-        root_dir = '/home/arbi/desktop/ILSVRC'
-        seq_dataset_val = ImageNetVID(root_dir, subset=('val'))
-    elif name == 'All':
-        root_dir_vid = '/home/arbi/desktop/ILSVRC'
-        seq_datasetVID = ImageNetVID(root_dir_vid, subset=('val'))
-        root_dir_got = args.train_path
-        seq_datasetGOT = GOT10k(root_dir_got, subset='val')
-        seq_dataset_val = util.data_split(seq_datasetVID, seq_datasetGOT)
-    print('seq_dataset_val', len(seq_dataset_val))
-
-    valid_z_transforms = transforms.Compose([
-        ToTensor()
-    ])
-    valid_x_transforms = transforms.Compose([
-        ToTensor()
-    ])
-
-    val_data  = TrainDataLoader(seq_dataset_val, valid_z_transforms, valid_x_transforms, name)
+    val_data  = TrainDataLoaderRGBT(seq_dataset_val, val_z_transforms, val_x_transforms, name)
     val_loader = DataLoader(    dataset    = val_data,
                                 batch_size = config.valid_batch_size,
                                 shuffle    = False,
@@ -103,7 +122,7 @@ def main():
 
     '''load weights'''
 
-    if not args.checkpoint_path == None:
+    if args.checkpoint_path:
         assert os.path.isfile(args.checkpoint_path), '{} is not valid checkpoint_path'.format(args.checkpoint_path)
         checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
         if 'model' in checkpoint.keys():
@@ -113,6 +132,7 @@ def main():
         torch.cuda.empty_cache()
         print('You are loading the model.load_state_dict')
 
+
     elif config.pretrained_model:
         checkpoint = torch.load(config.pretrained_model)
         # change name and load parameters
@@ -120,82 +140,152 @@ def main():
         model_dict = model.net.state_dict()
         model_dict.update(checkpoint)
         model.net.load_state_dict(model_dict)
-        #torch.cuda.empty_cache()
 
-    '''train phase'''
+
+
     train_closses, train_rlosses, train_tlosses = AverageMeter(), AverageMeter(), AverageMeter()
+    train_losses = AverageMeter()
     val_closses, val_rlosses, val_tlosses = AverageMeter(), AverageMeter(), AverageMeter()
+    val_losses = AverageMeter()
+
 
     train_val_plot = SavePlot(exp_name_dir, 'train_val_plot')
 
+    # from train.experimentrgbt import ExperimentRGBT
+    # experiment = ExperimentRGBT('/home/zuern/datasets/thermal_tracking/RGB-T234/',
+    #                    experiment_name='RGBT-Train',
+    #                    subset='val')
+    # json_path = os.path.join('experiments/{}'.format(args.experiment_name), 'parameters.json')
+    # assert os.path.isfile(json_path), ("No json configuration file found at {}".format(json_path))
+    # with open(json_path) as data_file:
+    #     params = json.load(data_file)
+
+
     for epoch in range(config.epoches):
+
         model.net.train()
-        if config.fix_former_3_layers:
-                util.freeze_layers(model.net)
+
         print('Train epoch {}/{}'.format(epoch+1, config.epoches))
         train_loss = []
+
         with tqdm(total=config.train_epoch_size) as progbar:
+
             for i, dataset in enumerate(train_loader):
 
-                closs, rloss, loss = model.step(epoch, dataset,anchors, i,  train=True)
+                if args.model == 'SiamRPN':
+
+                    closs, rloss, loss = model.step(epoch, dataset,anchors, i,  train=True)
+                    closs_ = closs.cpu().item()
+
+                    if np.isnan(closs_):
+                       sys.exit(0)
+
+                    train_closses.update(closs.cpu().item())
+                    train_rlosses.update(rloss.cpu().item())
+                    train_tlosses.update(loss.cpu().item())
+
+                    progbar.set_postfix(closs='{:05.3f}'.format(train_closses.avg),
+                                        rloss='{:05.5f}'.format(train_rlosses.avg),
+                                        tloss='{:05.3f}'.format(train_tlosses.avg))
+
+                    progbar.update()
+                    train_loss.append(train_tlosses.avg)
 
-                closs_ = closs.cpu().item()
+                else:
+                    loss = model.step(epoch, dataset, anchors, i, train=True)
 
-                if np.isnan(closs_):
-                   sys.exit(0)
+                    loss = loss.cpu().item()
 
-                train_closses.update(closs.cpu().item())
-                train_rlosses.update(rloss.cpu().item())
-                train_tlosses.update(loss.cpu().item())
+                    if np.isnan(loss):
+                        sys.exit(0)
 
-                progbar.set_postfix(closs='{:05.3f}'.format(train_closses.avg),
-                                    rloss='{:05.5f}'.format(train_rlosses.avg),
-                                    tloss='{:05.3f}'.format(train_tlosses.avg))
+                    train_losses.update(loss.cpu().item())
 
-                progbar.update()
-                train_loss.append(train_tlosses.avg)
+                    progbar.set_postfix(closs='{:05.3f}'.format(train_losses.avg))
 
-                if i >= config.train_epoch_size - 1:
+                    progbar.update()
+                    train_loss.append(train_losses.avg)
 
-                    '''save model'''
-                    model.save(model, exp_name_dir, epoch)
 
-                    break
+        print('saving model')
+        '''save model'''
+        model.save(model, exp_name_dir, epoch)
 
         train_loss = np.mean(train_loss)
 
         '''val phase'''
         val_loss = []
+
         with tqdm(total=config.val_epoch_size) as progbar:
             print('Val epoch {}/{}'.format(epoch+1, config.epoches))
             for i, dataset in enumerate(val_loader):
 
-                val_closs, val_rloss, val_tloss = model.step(epoch, dataset, anchors, train=False)
+                if args.model == 'SiamRPN':
+
+                    val_closs, val_rloss, val_tloss = model.step(epoch, dataset, anchors, train=False)
+                    closs_ = val_closs.cpu().item()
+
+                    if np.isnan(closs_):
+                        sys.exit(0)
+
+                    val_closses.update(val_closs.cpu().item())
+                    val_rlosses.update(val_rloss.cpu().item())
+                    val_tlosses.update(val_tloss.cpu().item())
 
-                closs_ = val_closs.cpu().item()
+                    progbar.set_postfix(closs='{:05.3f}'.format(val_closses.avg),
+                                        rloss='{:05.5f}'.format(val_rlosses.avg),
+                                        tloss='{:05.3f}'.format(val_tlosses.avg))
+                    progbar.update()
 
-                if np.isnan(closs_):
-                    sys.exit(0)
+                    val_loss.append(val_tlosses.avg)
 
-                val_closses.update(val_closs.cpu().item())
-                val_rlosses.update(val_rloss.cpu().item())
-                val_tlosses.update(val_tloss.cpu().item())
+                    if i >= config.val_epoch_size - 1:
+                        break
 
-                progbar.set_postfix(closs='{:05.3f}'.format(val_closses.avg),
-                                    rloss='{:05.5f}'.format(val_rlosses.avg),
-                                    tloss='{:05.3f}'.format(val_tlosses.avg))
+                else:
+                    loss = model.step(epoch, dataset, anchors, train=False)
+                    loss = loss.cpu().item()
 
-                progbar.update()
+                    if np.isnan(loss):
+                        sys.exit(0)
 
-                val_loss.append(val_tlosses.avg)
+                    val_losses.update(loss.cpu().item())
 
-                if i >= config.val_epoch_size - 1:
-                    break
+                    progbar.set_postfix(closs='{:05.3f}'.format(val_losses.avg))
+                    progbar.update()
+
+                    val_loss.append(val_losses.avg)
+
+                    if i >= config.val_epoch_size - 1:
+                        break
 
         val_loss = np.mean(val_loss)
         train_val_plot.update(train_loss, val_loss)
+
         print ('Train loss: {}, val loss: {}'.format(train_loss, val_loss))
 
+        wandb.log({'Train loss': train_loss,
+                   'Val loss': val_loss})
+
+        # # Get curves of training
+        # net_path = os.path.join('{}/model'.format(exp_name_dir), 'model_e%d.pth' % (epoch + 1))
+        # tracker = TrackerSiamRPNBIG(params, net_path)
+        #
+        # # remove sequences report dir and results dir:
+        # if os.path.exists(experiment.result_dir):
+        #     shutil.rmtree(experiment.result_dir)
+        # if os.path.exists(experiment.report_dir):
+        #     shutil.rmtree(experiment.report_dir)
+        #
+        # experiment.run(tracker)
+        # performance = experiment.report([tracker.name])
+        #
+        # wandb.log({"Success_curve": wandb.Histogram(performance[tracker.name]['overall']['succ_curve']),
+        #            "AO": performance[tracker.name]['overall']['ao'],
+        #            "SR": performance[tracker.name]['overall']['sr'],
+        #            "FPS": performance[tracker.name]['overall']['speed_fps']},
+        #           )
+
 
 if __name__ == '__main__':
     main()
