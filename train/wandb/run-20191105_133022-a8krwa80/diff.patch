diff --git a/tracking/data_loader.py b/tracking/data_loader.py
index 1a99c3c..7ceca07 100644
--- a/tracking/data_loader.py
+++ b/tracking/data_loader.py
@@ -7,6 +7,7 @@ import random
 import numpy as np
 from torchvision import datasets, transforms, utils
 
+
 class TrackerDataLoader(object):
 
     def get_instance_image(self, img, bbox, size_z, size_x, context_amount, img_mean=None):
diff --git a/tracking/network.py b/tracking/network.py
index 1704ab9..bf916a4 100644
--- a/tracking/network.py
+++ b/tracking/network.py
@@ -12,9 +12,106 @@ from torch import nn
 from config import config
 
 
+
+class _BatchNorm2d(nn.BatchNorm2d):
+
+    def __init__(self, num_features, *args, **kwargs):
+        super(_BatchNorm2d, self).__init__(
+            num_features, *args, eps=1e-6, momentum=0.05, **kwargs)
+
+
+class Dense(nn.Module):
+
+    def __init__(self, in_channels_rgb, in_channels_ir):
+        super(Dense, self).__init__()
+
+        #RGB BRANCH
+        self.conv1_rgb = nn.Sequential(
+            nn.Conv2d(in_channels_rgb, 96, 11, 2),
+            _BatchNorm2d(96),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1),
+            nn.MaxPool2d(3, 2))
+        self.conv2_rgb = nn.Sequential(
+            nn.Conv2d(96, 256, 5, 1, groups=2),
+            _BatchNorm2d(256),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1),
+            nn.MaxPool2d(3, 2))
+        self.conv3_rgb = nn.Sequential(
+            nn.Conv2d(256, 384, 3, 1),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1))
+        self.conv4_rgb = nn.Sequential(
+            nn.Conv2d(384, 384, 3, 1, groups=2),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True),
+            nn.Dropout2d(p=0.1))
+
+        #IR BRANCH
+        self.conv1_ir = nn.Sequential(
+            nn.Conv2d(in_channels_ir, 96, 11, 2),
+            _BatchNorm2d(96),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(3, 2))
+        self.conv2_ir = nn.Sequential(
+            nn.Conv2d(96, 256, 5, 1, groups=2),
+            _BatchNorm2d(256),
+            nn.ReLU(inplace=True),
+            nn.MaxPool2d(3, 2))
+        self.conv3_ir = nn.Sequential(
+            nn.Conv2d(256, 384, 3, 1),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True))
+        self.conv4_ir = nn.Sequential(
+            nn.Conv2d(384, 384, 3, 1, groups=2),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True))
+
+
+        # Shared layers
+        self.conv3_shared = nn.Sequential(
+            nn.Conv2d(512, 384, 5, 1, groups=2),
+            _BatchNorm2d(384),
+            nn.ReLU(inplace=True))
+        self.conv5 = nn.Sequential(
+            nn.Conv2d(1152, 512, 3, 1, groups=2),
+            _BatchNorm2d(512))
+
+
+
+    def forward(self, x_rgb, x_v):
+
+        # rgb branch
+        x_rgb = self.conv1_rgb(x_rgb)
+        x_rgb = self.conv2_rgb(x_rgb)
+        # ir branch
+        x_v = self.conv1_ir(x_v)
+        x_v = self.conv2_ir(x_v)
+
+        x_shared = torch.cat((x_rgb, x_v), 1)
+        x_shared = self.conv3_shared(x_shared)
+
+
+        x_rgb = self.conv3_rgb(x_rgb)
+        x_rgb = self.conv4_rgb(x_rgb)
+
+
+        x_v = self.conv3_ir(x_v)
+        x_v = self.conv4_ir(x_v)
+        x_final = torch.cat((x_rgb, x_shared, x_v), 1)
+        x_final = self.conv5(x_final)
+
+        return x_final
+
+
+
 class SiameseAlexNet(nn.Module):
     def __init__(self, ):
         super(SiameseAlexNet, self).__init__()
+
+
         self.featureExtract = nn.Sequential(
             nn.Conv2d(3, 96, 11, stride=2),
             nn.BatchNorm2d(96),
@@ -33,6 +130,8 @@ class SiameseAlexNet(nn.Module):
             nn.Conv2d(384, 256, 3),
             nn.BatchNorm2d(256),
         )
+
+
         self.anchor_num = config.anchor_num
         self.input_size = config.detection_img_size
         self.score_displacement = int((self.input_size - config.template_img_size) / config.total_stride)
@@ -54,7 +153,11 @@ class SiameseAlexNet(nn.Module):
                 m.bias.data.zero_()
 
     def forward(self, template, detection):
+
+
         N = template.size(0)
+
+
         template_feature = self.featureExtract(template)
         detection_feature = self.featureExtract(detection)
 
@@ -99,3 +202,81 @@ class SiameseAlexNet(nn.Module):
             F.conv2d(conv_reg, self.reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,
                                                                    self.score_displacement + 1))
         return pred_score, pred_regression
+
+
+class SiameseAlexNetMultimodal(nn.Module):
+    def __init__(self, ):
+        super(SiameseAlexNetMultimodal, self).__init__()
+
+        self.featureExtract = Dense()
+
+        self.anchor_num = config.anchor_num
+        self.input_size = config.detection_img_size
+        self.score_displacement = int((self.input_size - config.template_img_size) / config.total_stride)
+        self.conv_cls1 = nn.Conv2d(256, 256 * 2 * self.anchor_num, kernel_size=3, stride=1, padding=0)
+        self.conv_r1 = nn.Conv2d(256, 256 * 4 * self.anchor_num, kernel_size=3, stride=1, padding=0)
+
+        self.conv_cls2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0)
+        self.conv_r2 = nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=0)
+        self.regress_adjust = nn.Conv2d(4 * self.anchor_num, 4 * self.anchor_num, 1)
+
+    def init_weights(self):
+        for m in self.modules():
+            if isinstance(m, nn.Conv2d):
+                # nn.init.kaiming_normal_(m.weight.data, mode='fan_out', nonlinearity='relu')
+                nn.init.normal_(m.weight.data, std=0.0005)
+                nn.init.normal_(m.bias.data, std=0.0005)
+            elif isinstance(m, nn.BatchNorm2d):
+                m.weight.data.fill_(1)
+                m.bias.data.zero_()
+
+    def forward(self, template_rgb, detection_rgb, template_ir, detection_ir):
+
+        N = template_rgb.size(0)
+
+        template_feature = self.featureExtract(template_rgb, template_ir)
+        detection_feature = self.featureExtract(detection_rgb, detection_ir)
+
+
+        kernel_score = self.conv_cls1(template_feature).view(N, 2 * self.anchor_num, 256, 4, 4)
+        kernel_regression = self.conv_r1(template_feature).view(N, 4 * self.anchor_num, 256, 4, 4)
+        conv_score = self.conv_cls2(detection_feature)
+        conv_regression = self.conv_r2(detection_feature)
+
+        conv_scores = conv_score.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        score_filters = kernel_score.reshape(-1, 256, 4, 4)
+        pred_score = F.conv2d(conv_scores, score_filters, groups=N).reshape(N, 10, self.score_displacement + 1,
+                                                                            self.score_displacement + 1)
+
+        conv_reg = conv_regression.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        reg_filters = kernel_regression.reshape(-1, 256, 4, 4)
+        pred_regression = self.regress_adjust(
+            F.conv2d(conv_reg, reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,
+                                                              self.score_displacement + 1))
+        return pred_score, pred_regression
+
+    def track_init(self, template):
+        N = template.size(0)
+        template_feature = self.featureExtract(template)
+
+        kernel_score = self.conv_cls1(template_feature).view(N, 2 * self.anchor_num, 256, 4, 4)
+        kernel_regression = self.conv_r1(template_feature).view(N, 4 * self.anchor_num, 256, 4, 4)
+        self.score_filters = kernel_score.reshape(-1, 256, 4, 4)
+        self.reg_filters = kernel_regression.reshape(-1, 256, 4, 4)
+
+    def track(self, detection):
+        N = detection.size(0)
+        detection_feature = self.featureExtract(detection)
+
+        conv_score = self.conv_cls2(detection_feature)
+        conv_regression = self.conv_r2(detection_feature)
+
+        conv_scores = conv_score.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        pred_score = F.conv2d(conv_scores, self.score_filters, groups=N).reshape(N, 10, self.score_displacement + 1,
+                                                                                 self.score_displacement + 1)
+        conv_reg = conv_regression.reshape(1, -1, self.score_displacement + 4, self.score_displacement + 4)
+        pred_regression = self.regress_adjust(
+            F.conv2d(conv_reg, self.reg_filters, groups=N).reshape(N, 20, self.score_displacement + 1,
+                                                                   self.score_displacement + 1))
+        return pred_score, pred_regression
+
diff --git a/tracking/run_tracking.py b/tracking/run_tracking.py
index 68b9ed1..502c436 100644
--- a/tracking/run_tracking.py
+++ b/tracking/run_tracking.py
@@ -4,19 +4,21 @@ from siamRPNBIG import TrackerSiamRPNBIG
 import argparse
 import os
 import json
+from train.experimentrgbt import ExperimentRGBT
+
 
-parser = argparse.ArgumentParser(description='PyTorch SiameseRPN Tracking')
 
-parser.add_argument('--tracker_path', default='/home/arbi/desktop/data', metavar='DIR',help='path to dataset')
+parser = argparse.ArgumentParser(description='PyTorch SiameseRPN Tracking')
+parser.add_argument('--tracker_path', default='/home/zuern/datasets/tracking/GOT10k', metavar='DIR',help='path to dataset')
 parser.add_argument('--experiment_name', default='default', metavar='DIR',help='path to weight')
-parser.add_argument('--net_path', default='../train/experiments/default/model/model_e17.pth', metavar='DIR',help='path to weight')
-# ../train/experiments/default/model/model_e1.pth # ../model.pth #../siamrpn_7.pth
-# /Users/arbi/Desktop # /home/arbi/desktop/GOT-10k
-# /media/arbi/9132EE0B9756C987/dataset/GOT-10k/full_data
-parser.add_argument('--visualize', default=True, help='visualize')
+parser.add_argument('--net_path', default='../train/experiments/SiamRPN/model/model_e1.pth', metavar='DIR',help='path to weight')
+
+parser.add_argument('--visualize', default=False, help='visualize')
 
 args = parser.parse_args()
 
+
+
 if __name__ == '__main__':
 
     """Load the parameters from json file"""
@@ -26,8 +28,11 @@ if __name__ == '__main__':
         params = json.load(data_file)
 
     '''setup tracker'''
+
+
     tracker = TrackerSiamRPNBIG(params, args.net_path)
 
+
     '''setup experiments'''
     # 7 datasets with different versions
     '''
@@ -54,11 +59,19 @@ if __name__ == '__main__':
                     report_dir='experiments/{}/reports'.format(args.experiment_name))
 
     '''
-    experiments = ExperimentOTB('/home/arbi/desktop/data', version=2015,
-                    result_dir='experiments/{}/OTB2015resultsGOT-10k_42'.format(args.experiment_name),
-                    report_dir='experiments/{}/OTB2015reportsGOT-10k_42'.format(args.experiment_name))
+    # experiments = ExperimentOTB('/home/arbi/desktop/data', version=2015,
+    #                 result_dir='experiments/{}/OTB2015resultsGOT-10k_42'.format(args.experiment_name),
+    #                 report_dir='experiments/{}/OTB2015reportsGOT-10k_42'.format(args.experiment_name))
+
+    experiments = ExperimentGOT10k(args.tracker_path, subset='val',
+                    result_dir='experiments/{}/results'.format(args.experiment_name),
+                    report_dir='experiments/{}/reports'.format(args.experiment_name))
+
+    experiments = ExperimentRGBT('/home/zuern/datasets/thermal_tracking/RGB-T234/',
+                       experiment_name='RGBT-Train',
+                       subset='val')
 
 
     '''run experiments'''
     experiments.run(tracker, visualize = args.visualize)
-    experiments.report([tracker.name])
+    experiments.report([tracker.name], return_report=False)
diff --git a/tracking/siamRPNBIG.py b/tracking/siamRPNBIG.py
index c304b83..5ff8230 100644
--- a/tracking/siamRPNBIG.py
+++ b/tracking/siamRPNBIG.py
@@ -2,17 +2,17 @@ import cv2
 import torch
 import numpy as np
 import torch.nn as nn
-from util import util
+from tracking.util import util
 import torch.nn.functional as F
-from config import TrackerConfig
 import torchvision.transforms as transforms
-from custom_transforms import ToTensor
-from config import config
+from tracking.custom_transforms import ToTensor
+from tracking.config import config
 from torch.autograd import Variable
 from got10k.trackers import Tracker
-from network import SiameseAlexNet
-from data_loader import TrackerDataLoader
-from PIL import Image, ImageOps, ImageStat, ImageDraw
+from tracking.network import SiameseAlexNet
+from tracking.data_loader import TrackerDataLoader
+
+
 
 class SiamRPN(nn.Module):
 
@@ -73,7 +73,10 @@ class SiamRPN(nn.Module):
 
         return out_reg, out_cls
 
+
+
 class TrackerSiamRPNBIG(Tracker):
+
     def __init__(self, params, model_path = None, **kargs):
         super(TrackerSiamRPNBIG, self).__init__(name='SiamRPN', is_deterministic=True)
 
diff --git a/train/config.py b/train/config.py
index 76c65f1..b0195ef 100644
--- a/train/config.py
+++ b/train/config.py
@@ -37,7 +37,8 @@ class Config(object):
     out_feature = 19
     max_inter   = 80
     fix_former_3_layers = True
-    pretrained_model =  '/home/arbi/Загрузки/alexnet.pth' # '/home/arbi/desktop/alexnet.pth' # # '/Users/arbi/Desktop/alexnet.pth'
+    # pretrained_model =  '/home/arbi/Загрузки/alexnet.pth' # '/home/arbi/desktop/alexnet.pth' # # '/Users/arbi/Desktop/alexnet.pth'
+    pretrained_model =  None
 
     total_stride = 8
     anchor_total_stride = total_stride
diff --git a/train/experiments/default/parameters.json b/train/experiments/default/parameters.json
deleted file mode 100644
index 2a0a6ff..0000000
--- a/train/experiments/default/parameters.json
+++ /dev/null
@@ -1,14 +0,0 @@
-{
-    "template_img_size": 127,
-    "detection_img_size": 271,
-    "stride": 8,
-    "lr": 1e-5,
-    "epoches": 200,
-    "weight_decay": 0.0005,
-    "momentum": 0.9,
-    "context": 0.5,
-    "ratios": [0.33, 0.5, 1, 2, 3],
-    "scales": [8],
-    "penalty_k": 0.055,
-    "window_influence": 0.42
-}
diff --git a/train/net.py b/train/net.py
index 611ae89..440e264 100644
--- a/train/net.py
+++ b/train/net.py
@@ -11,22 +11,31 @@ import matplotlib.pyplot as plt
 import torch.nn.functional as F
 from config import config
 from got10k.trackers import Tracker
+
+
 from network import SiameseAlexNet
+from network_siamfc import Net, SiamFC, AlexNetV3
+
+
 from loss import rpn_smoothL1, rpn_cross_entropy_balance
 
 class TrackerSiamRPN(Tracker):
 
-    def __init__(self, net_path=None, **kargs):
+    def __init__(self, net_path=None, modality=1, **kargs):
         super(TrackerSiamRPN, self).__init__(
             name='SiamRPN', is_deterministic=True)
 
         '''setup GPU device if available'''
         self.cuda   = torch.cuda.is_available()
         self.device = torch.device('cuda:0' if self.cuda else 'cpu')
+        self.modality = modality
 
         '''setup model'''
-        self.net = SiameseAlexNet()
-        #self.net.init_weights()
+        if self.modality == 1:
+            self.net = SiameseAlexNet()
+        else:
+            self.net = SiameseAlexNetMultimodal()
+
 
         if net_path is not None:
             self.net.load_state_dict(torch.load(
@@ -41,6 +50,7 @@ class TrackerSiamRPN(Tracker):
             momentum     = config.momentum,
             weight_decay = config.weight_decay)
 
+
     def step(self, epoch, dataset, anchors, i = 0,  train=True):
 
         if train:
@@ -76,30 +86,6 @@ class TrackerSiamRPN(Tracker):
 
         loss = cls_loss + config.lamb * reg_loss
 
-        '''anchors_show = anchors
-        exem_img = template[0].cpu().numpy().transpose(1, 2, 0)  # (127, 127, 3)
-        #cv2.imwrite('exem_img.png', exem_img)
-
-        inst_img = detection[0].cpu().numpy().transpose(1, 2, 0) # (255, 255, 3)
-        #cv2.imwrite('inst_img.png', inst_img)
-
-
-
-        topk = 1
-        cls_pred = F.softmax(pred_conf, dim=2)[0, :, 1]
-
-        topk_box = util.get_topk_box(cls_pred, pred_offset[0], anchors_show, topk=topk)
-        img_box = util.add_box_img(inst_img, topk_box, color=(0, 0, 255))
-
-        cv2.imwrite('pred_inst.png', img_box)
-
-        cls_pred = conf_target[0]
-        gt_box = util.get_topk_box(cls_pred, regression_target[0], anchors_show)
-        #print('gt_box', gt_box)
-        img_box = util.add_box_img(img_box, gt_box, color=(255, 0, 0), x = 1, y = 1)
-        #print('gt_box', gt_box)
-        cv2.imwrite('pred_inst_gt.png', img_box)'''
-
         if train:
             self.optimizer.zero_grad()
             loss.backward()
@@ -118,61 +104,88 @@ class TrackerSiamRPN(Tracker):
         net_path = os.path.join(model_save_dir_pth, 'model_e%d.pth' % (epoch + 1))
         torch.save(model.net.state_dict(), net_path)
 
-'''class SiamRPN(nn.Module):
-
-    def __init__(self, anchor_num = 5):
-        super(SiamRPN, self).__init__()
-
-        self.anchor_num = anchor_num
-        self.feature = nn.Sequential(
-            # conv1
-            nn.Conv2d(3, 64, kernel_size = 11, stride = 2),
-            nn.BatchNorm2d(64),
-            nn.ReLU(inplace = True),
-            nn.MaxPool2d(kernel_size = 3, stride = 2),
-            # conv2
-            nn.Conv2d(64, 192, kernel_size = 5),
-            nn.BatchNorm2d(192),
-            nn.ReLU(inplace=True),
-            nn.MaxPool2d(kernel_size = 3, stride = 2),
-            # conv3
-            nn.Conv2d(192, 384, kernel_size = 3),
-            nn.BatchNorm2d(384),
-            nn.ReLU(inplace = True),
-            # conv4
-            nn.Conv2d(384, 256, kernel_size = 3),
-            nn.BatchNorm2d(256),
-            nn.ReLU(inplace = True),
-            # conv5
-            nn.Conv2d(256, 256, kernel_size = 3),
-            nn.BatchNorm2d(256))
-
-        self.conv_reg_z = nn.Conv2d(256, 256 * 4 * self.anchor_num, 3, 1)
-        self.conv_reg_x = nn.Conv2d(256, 256, 3)
-        self.conv_cls_z = nn.Conv2d(256, 256 * 2 * anchor_num, 3, 1)
-        self.conv_cls_x = nn.Conv2d(256, 256, 3)
-        self.adjust_reg = nn.Conv2d(4 * anchor_num, 4 * anchor_num*1, 1)
-
-    def forward(self, z, x):
-        return self.inference(x, *self.learn(z))
-
-    def learn(self, z):
-        z = self.feature(z)
-        kernel_reg = self.conv_reg_z(z)
-        kernel_cls = self.conv_cls_z(z)
-
-        k = kernel_reg.size()[-1]
-        kernel_reg = kernel_reg.view(4 * self.anchor_num, 256, k, k)
-        kernel_cls = kernel_cls.view(2 * self.anchor_num, 256, k, k)
-
-        return kernel_reg, kernel_cls
-
-    def inference(self, x, kernel_reg, kernel_cls):
-        x = self.feature(x)
-        x_reg = self.conv_reg_x(x)
-        x_cls = self.conv_cls_x(x)
-
-        out_reg = self.adjust_reg(F.conv2d(x_reg, kernel_reg))
-        out_cls = F.conv2d(x_cls, kernel_cls)
-
-        return out_reg, out_cls'''
+
+class TrackerSiamFC(Tracker):
+
+    def __init__(self, net_path=None, **kwargs):
+
+        super(TrackerSiamFC, self).__init__('SiamFC', True)
+
+        # setup GPU device if available
+        self.cuda = torch.cuda.is_available()
+        self.device = torch.device('cuda:0' if self.cuda else 'cpu')
+
+        print('Training on device {}'.format(self.device))
+
+        '''setup GPU device if available'''
+        self.cuda = torch.cuda.is_available()
+        self.device = torch.device('cuda:0' if self.cuda else 'cpu')
+
+
+
+        # setup model
+        self.net = Net(
+            backbone=AlexNetV3(),
+            head=SiamFC(self.cfg.out_scale))
+
+        # load checkpoint if provided
+        if net_path is not None:
+            self.net.load_state_dict(torch.load(
+                net_path, map_location=lambda storage, loc: storage))
+        self.net = self.net.to(self.device)
+
+
+        # setup criterion
+        from network_siamfc import BalancedLoss
+        self.criterion = BalancedLoss()
+
+        # setup optimizer
+        self.optimizer = torch.optim.SGD(
+            self.net.parameters(),
+            lr=self.cfg.initial_lr,
+            weight_decay=self.cfg.weight_decay,
+            momentum=self.cfg.momentum)
+
+
+    def step(self, epoch, dataset, anchors, i = 0,  train=True):
+
+        # TODO: implement further
+
+
+        if train:
+            self.net.train()
+        else:
+            self.net.eval()
+
+        template, detection, regression_target, conf_target = dataset
+
+        if self.cuda:
+            template, detection = template.cuda(), detection.cuda()
+            regression_target, conf_target = regression_target.cuda(), conf_target.cuda()
+
+        pred_score, pred_regression = self.net(template, detection)
+
+        pred_conf = pred_score.reshape(-1, 2, config.size).permute(0, 2, 1)
+
+        pred_offset = pred_regression.reshape(-1, 4, config.size).permute(0, 2, 1)
+
+        cls_loss = self.criterion(pred_conf,conf_target)
+
+        reg_loss = rpn_smoothL1(pred_offset,
+                                regression_target,
+                                conf_target,
+                                config.num_pos,
+                                ohem=config.ohem_reg)
+
+        loss = cls_loss + config.lamb * reg_loss
+
+        if train:
+            self.optimizer.zero_grad()
+            loss.backward()
+            torch.nn.utils.clip_grad_norm_(self.net.parameters(), config.clip)
+            self.optimizer.step()
+
+        return cls_loss, reg_loss, loss
+
+
+
diff --git a/train/train_siamrpn.py b/train/train_siamrpn.py
index 42c0ffd..1df2a10 100644
--- a/train/train_siamrpn.py
+++ b/train/train_siamrpn.py
@@ -10,49 +10,78 @@ import numpy as np
 from tqdm import tqdm
 from torch.nn import init
 from config import config
-from net import TrackerSiamRPN
+from net import TrackerSiamRPN, TrackerSiamFC
+
 from data import TrainDataLoader
+from data_rgbt import TrainDataLoaderRGBT
 from torch.utils.data import DataLoader
+
 from util import util, AverageMeter, SavePlot
 from got10k.datasets import ImageNetVID, GOT10k
 from torchvision import datasets, transforms, utils
 from custom_transforms import Normalize, ToTensor, RandomStretch, \
     RandomCrop, CenterCrop, RandomBlur, ColorAug
+import wandb
+import shutil
+
+from tracking.siamRPNBIG import TrackerSiamRPNBIG
+
+
+from train.experimentrgbt import RGBTSequence
+
 
 torch.manual_seed(1234) # config.seed
 
 
 parser = argparse.ArgumentParser(description='PyTorch SiameseRPN Training')
 
-parser.add_argument('--train_path', default='/home/arbi/desktop/GOT-10k', metavar='DIR',help='path to dataset')
-parser.add_argument('--experiment_name', default='default', metavar='DIR',help='path to weight')
+parser.add_argument('--train_path', default='/home/zuern/datasets/tracking/GOT10k', metavar='DIR',help='path to dataset')
+parser.add_argument('--experiment_name', default='SiamRPN', metavar='DIR',help='path to weight')
 parser.add_argument('--checkpoint_path', default=None, help='resume')
-# /home/arbi/desktop/GOT-10k # /Users/arbi/Desktop # /home/arbi/desktop/ILSVRC
-# 'experiments/default/model/model_e1.pth'
+
+
+
 def main():
 
+
+
     '''parameter initialization'''
     args = parser.parse_args()
     exp_name_dir = util.experiment_name_dir(args.experiment_name)
 
+
+    wandb.init(project="SiameseX", reinit=True)
+
+
     '''model on gpu'''
-    model = TrackerSiamRPN()
+    model = TrackerSiamRPN(modality=1)
+    # model = TrackerSiamFC()
 
     '''setup train data loader'''
-    name = 'GOT-10k'
-    assert name in ['VID', 'GOT-10k', 'All']
+
+    name = 'RGBT-234'
+
+
+    assert name in ['VID', 'GOT-10k', 'All', 'RGBT-234']
+
     if name == 'GOT-10k':
         root_dir = args.train_path
         seq_dataset = GOT10k(root_dir, subset='train')
     elif name == 'VID':
-        root_dir = '/home/arbi/desktop/ILSVRC'
+        root_dir = '/home/zuern/datasets/tracking/VID/ILSVRC'
         seq_dataset = ImageNetVID(root_dir, subset=('train'))
+
+    elif name == 'RGBT-234':
+        seq_dataset = RGBTSequence('/home/zuern/datasets/thermal_tracking/RGB-T234/', subset='train')
+
+
     elif name == 'All':
         root_dir_vid = '/home/arbi/desktop/ILSVRC'
         seq_datasetVID = ImageNetVID(root_dir_vid, subset=('train'))
         root_dir_got = args.train_path
         seq_datasetGOT = GOT10k(root_dir_got, subset='train')
         seq_dataset = util.data_split(seq_datasetVID, seq_datasetGOT)
+
     print('seq_dataset', len(seq_dataset))
 
     train_z_transforms = transforms.Compose([
@@ -62,7 +91,7 @@ def main():
         ToTensor()
     ])
 
-    train_data  = TrainDataLoader(seq_dataset, train_z_transforms, train_x_transforms, name)
+    train_data  = TrainDataLoaderRGBT(seq_dataset, train_z_transforms, train_x_transforms, name)
     anchors = train_data.anchors
     train_loader = DataLoader(  dataset    = train_data,
                                 batch_size = config.train_batch_size,
@@ -71,14 +100,22 @@ def main():
                                 pin_memory = True)
 
     '''setup val data loader'''
-    name = 'GOT-10k'
-    assert name in ['VID', 'GOT-10k', 'All']
+    name = 'RGBT-234'
+
+
+    assert name in ['VID', 'GOT-10k', 'All', 'RGBT-234']
     if name == 'GOT-10k':
         root_dir = args.train_path
         seq_dataset_val = GOT10k(root_dir, subset='val')
     elif name == 'VID':
         root_dir = '/home/arbi/desktop/ILSVRC'
         seq_dataset_val = ImageNetVID(root_dir, subset=('val'))
+
+
+    elif name == 'RGBT-234':
+        seq_dataset_val = RGBTSequence('/home/zuern/datasets/thermal_tracking/RGB-T234/', subset='eval')
+
+
     elif name == 'All':
         root_dir_vid = '/home/arbi/desktop/ILSVRC'
         seq_datasetVID = ImageNetVID(root_dir_vid, subset=('val'))
@@ -94,7 +131,7 @@ def main():
         ToTensor()
     ])
 
-    val_data  = TrainDataLoader(seq_dataset_val, valid_z_transforms, valid_x_transforms, name)
+    val_data  = TrainDataLoaderRGBT(seq_dataset_val, valid_z_transforms, valid_x_transforms, name)
     val_loader = DataLoader(    dataset    = val_data,
                                 batch_size = config.valid_batch_size,
                                 shuffle    = False,
@@ -103,7 +140,7 @@ def main():
 
     '''load weights'''
 
-    if not args.checkpoint_path == None:
+    if args.checkpoint_path:
         assert os.path.isfile(args.checkpoint_path), '{} is not valid checkpoint_path'.format(args.checkpoint_path)
         checkpoint = torch.load(args.checkpoint_path, map_location='cpu')
         if 'model' in checkpoint.keys():
@@ -128,13 +165,28 @@ def main():
 
     train_val_plot = SavePlot(exp_name_dir, 'train_val_plot')
 
+
+
+    from train.experimentrgbt import ExperimentRGBT
+    experiment = ExperimentRGBT('/home/zuern/datasets/thermal_tracking/RGB-T234/',
+                       experiment_name='RGBT-Train',
+                       subset='val')
+    json_path = os.path.join('experiments/{}'.format(args.experiment_name), 'parameters.json')
+    assert os.path.isfile(json_path), ("No json configuration file found at {}".format(json_path))
+    with open(json_path) as data_file:
+        params = json.load(data_file)
+
+
     for epoch in range(config.epoches):
+
         model.net.train()
         if config.fix_former_3_layers:
                 util.freeze_layers(model.net)
         print('Train epoch {}/{}'.format(epoch+1, config.epoches))
         train_loss = []
+
         with tqdm(total=config.train_epoch_size) as progbar:
+
             for i, dataset in enumerate(train_loader):
 
                 closs, rloss, loss = model.step(epoch, dataset,anchors, i,  train=True)
@@ -155,13 +207,15 @@ def main():
                 progbar.update()
                 train_loss.append(train_tlosses.avg)
 
-                if i >= config.train_epoch_size - 1:
-
-                    '''save model'''
-                    model.save(model, exp_name_dir, epoch)
 
+                if i > 10:
                     break
 
+
+        print('saving model')
+        '''save model'''
+        model.save(model, exp_name_dir, epoch)
+
         train_loss = np.mean(train_loss)
 
         '''val phase'''
@@ -192,10 +246,36 @@ def main():
                 if i >= config.val_epoch_size - 1:
                     break
 
+
+
         val_loss = np.mean(val_loss)
         train_val_plot.update(train_loss, val_loss)
         print ('Train loss: {}, val loss: {}'.format(train_loss, val_loss))
 
+        wandb.log({'Train loss': train_loss,
+                   'Val loss': val_loss},
+                    step = epoch + 1)
+
+
+        # Get curves of training
+        net_path = os.path.join('{}/model'.format(exp_name_dir), 'model_e%d.pth' % (epoch + 1))
+        tracker = TrackerSiamRPNBIG(params, net_path)
+
+        # remove sequences report dir and results dir:
+        if os.path.exists(experiment.result_dir):
+            shutil.rmtree(experiment.result_dir)
+        if os.path.exists(experiment.report_dir):
+            shutil.rmtree(experiment.report_dir)
+
+        experiment.run(tracker)
+        performance = experiment.report([tracker.name])
+
+        wandb.log({"Success_curve": wandb.Histogram(performance[tracker.name]['overall']['succ_curve']),
+                   "AO": performance[tracker.name]['overall']['ao'],
+                   "SR": performance[tracker.name]['overall']['sr'],
+                   "FPS": performance[tracker.name]['overall']['speed_fps']},
+                  step=epoch + 1)
+
 
 if __name__ == '__main__':
     main()
